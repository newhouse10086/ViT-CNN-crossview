#!/usr/bin/env python3
"""
æµ‹è¯•FSRA-VMKåˆ›æ–°æ¶æ„
Vision Mamba Kolmogorov Network - åŸºäº2024å¹´æœ€æ–°ç¥ç»ç½‘ç»œæ¨¡å—
"""

import sys
from pathlib import Path
import torch
import torch.nn as nn
import numpy as np
import time
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.models.fsra_vmk_improved import (
    create_fsra_vmk_model,
    VisionMambaEncoder,
    VisionMambaBlock,
    KANLinear,
    KolmogorovArnoldAttention,
    ConvNeXtV2FusionModule,
    ConvNeXtV2Block,
    GlobalResponseNorm,
    BidirectionalCrossViewAlignment
)


def test_kan_linear():
    """æµ‹è¯•Kolmogorov-Arnold Networksçº¿æ€§å±‚"""
    print("ğŸ§ª Testing Kolmogorov-Arnold Networks (KAN) Linear Layer...")
    
    batch_size, in_features, out_features = 4, 256, 128
    grid_size = 5
    
    # åˆ›å»ºKANçº¿æ€§å±‚
    kan_layer = KANLinear(in_features, out_features, grid_size)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, in_features)
    
    # å‰å‘ä¼ æ’­
    output = kan_layer(x)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… Grid size: {grid_size}")
    print(f"âœ… B-spline bases computed successfully")
    
    # éªŒè¯è¾“å‡ºç»´åº¦
    assert output.shape == (batch_size, out_features)
    
    # æµ‹è¯•æ¢¯åº¦è®¡ç®—
    loss = output.sum()
    loss.backward()
    
    print(f"âœ… Gradient computation successful")
    print(f"âœ… KAN spline weight grad: {kan_layer.spline_weight.grad is not None}")
    print("âœ… KAN Linear Layer test passed!\n")


def test_kolmogorov_arnold_attention():
    """æµ‹è¯•åŸºäºKANçš„æ³¨æ„åŠ›æœºåˆ¶"""
    print("ğŸ§ª Testing Kolmogorov-Arnold Attention (KAA)...")
    
    batch_size, seq_len, dim = 2, 64, 256  # 8x8 feature map
    num_heads = 8
    H, W = 8, 8
    
    # åˆ›å»ºKANæ³¨æ„åŠ›æ¨¡å—
    kan_attention = KolmogorovArnoldAttention(dim, num_heads)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, dim)
    
    # å‰å‘ä¼ æ’­
    output = kan_attention(x, H, W)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… Number of heads: {num_heads}")
    print(f"âœ… Feature map size: {H}x{W}")
    
    # éªŒè¯è¾“å‡º
    assert output.shape == x.shape
    print("âœ… Kolmogorov-Arnold Attention test passed!\n")


def test_vision_mamba_block():
    """æµ‹è¯•Vision Mambaå—"""
    print("ğŸ§ª Testing Vision Mamba Block...")
    
    batch_size, seq_len, dim = 2, 256, 384  # 16x16 patches
    H, W = 16, 16
    d_state = 16
    
    # åˆ›å»ºVision Mambaå—
    mamba_block = VisionMambaBlock(dim, d_state)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, dim)
    
    # å‰å‘ä¼ æ’­
    output = mamba_block(x, H, W)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… State dimension: {d_state}")
    print(f"âœ… Selective scan computed")
    
    # éªŒè¯æ®‹å·®è¿æ¥
    assert output.shape == x.shape
    print("âœ… Vision Mamba Block test passed!\n")


def test_vision_mamba_encoder():
    """æµ‹è¯•Vision Mambaç¼–ç å™¨"""
    print("ğŸ§ª Testing Vision Mamba Encoder (VME)...")
    
    batch_size = 2
    img_size = 256
    patch_size = 16
    embed_dim = 384
    depth = 6  # å‡å°‘æ·±åº¦ä»¥èŠ‚çœæµ‹è¯•æ—¶é—´
    
    # åˆ›å»ºVision Mambaç¼–ç å™¨
    vme = VisionMambaEncoder(
        img_size=img_size,
        patch_size=patch_size,
        embed_dim=embed_dim,
        depth=depth
    )
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, 3, img_size, img_size)
    
    # å‰å‘ä¼ æ’­
    features = vme(x)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Number of patches: {vme.num_patches}")
    print(f"âœ… Embed dimension: {embed_dim}")
    print(f"âœ… Mamba depth: {depth}")
    
    # éªŒè¯å¤šå°ºåº¦ç‰¹å¾
    expected_keys = ['S1', 'S2', 'S3', 'S4', 'mamba_features']
    for key in expected_keys:
        assert key in features, f"Missing feature key: {key}"
        print(f"âœ… {key}: {features[key].shape}")
    
    print("âœ… Vision Mamba Encoder test passed!\n")


def test_convnext_v2_block():
    """æµ‹è¯•ConvNeXt V2å—"""
    print("ğŸ§ª Testing ConvNeXt V2 Block...")
    
    batch_size, channels, height, width = 2, 256, 32, 32
    
    # åˆ›å»ºConvNeXt V2å—
    convnext_block = ConvNeXtV2Block(channels)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, channels, height, width)
    
    # å‰å‘ä¼ æ’­
    output = convnext_block(x)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… Global Response Norm applied")
    print(f"âœ… Layer Scale enabled")
    
    # éªŒè¯æ®‹å·®è¿æ¥
    assert output.shape == x.shape
    print("âœ… ConvNeXt V2 Block test passed!\n")


def test_global_response_norm():
    """æµ‹è¯•Global Response Normalization"""
    print("ğŸ§ª Testing Global Response Normalization (GRN)...")
    
    batch_size, height, width, channels = 2, 16, 16, 512
    
    # åˆ›å»ºGRNæ¨¡å—
    grn = GlobalResponseNorm(channels)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ® (NHWCæ ¼å¼)
    x = torch.randn(batch_size, height, width, channels)
    
    # å‰å‘ä¼ æ’­
    output = grn(x)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… Global response computed")
    
    assert output.shape == x.shape
    print("âœ… Global Response Normalization test passed!\n")


def test_convnext_v2_fusion():
    """æµ‹è¯•ConvNeXt V2èåˆæ¨¡å—"""
    print("ğŸ§ª Testing ConvNeXt V2 Fusion Module (CFM)...")
    
    batch_size = 2
    in_channels = 384
    fusion_channels = 256
    
    # åˆ›å»ºèåˆæ¨¡å—
    cfm = ConvNeXtV2FusionModule(in_channels, fusion_channels)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ® (æ¨¡æ‹ŸMambaç‰¹å¾)
    mamba_features = {
        'S1': torch.randn(batch_size, in_channels, 16, 16),
        'S2': torch.randn(batch_size, in_channels, 8, 8),
        'S3': torch.randn(batch_size, in_channels, 4, 4),
        'S4': torch.randn(batch_size, in_channels, 2, 2)
    }
    
    # å‰å‘ä¼ æ’­
    fused_output = cfm(mamba_features)
    
    print(f"âœ… Input features: {list(mamba_features.keys())}")
    for key, feat in mamba_features.items():
        print(f"    {key}: {feat.shape}")
    print(f"âœ… Fused output: {fused_output.shape}")
    print(f"âœ… ConvNeXt V2 blocks applied")
    
    # éªŒè¯è¾“å‡ºå°ºå¯¸
    expected_shape = (batch_size, fusion_channels, 16, 16)  # ä»¥S1ä¸ºåŸºå‡†
    assert fused_output.shape == expected_shape
    print("âœ… ConvNeXt V2 Fusion Module test passed!\n")


def test_bidirectional_cross_view_alignment():
    """æµ‹è¯•åŒå‘è·¨è§†è§’å¯¹é½"""
    print("ğŸ§ª Testing Bidirectional Cross-View Alignment (BCVA)...")
    
    batch_size, feature_dim, height, width = 2, 256, 16, 16
    num_heads = 8
    
    # åˆ›å»ºåŒå‘å¯¹é½æ¨¡å—
    bcva = BidirectionalCrossViewAlignment(feature_dim, num_heads)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    sat_features = torch.randn(batch_size, feature_dim, height, width)
    uav_features = torch.randn(batch_size, feature_dim, height, width)
    
    # å‰å‘ä¼ æ’­
    aligned_features = bcva(sat_features, uav_features)
    
    print(f"âœ… Satellite input: {sat_features.shape}")
    print(f"âœ… UAV input: {uav_features.shape}")
    print(f"âœ… Aligned output: {aligned_features.shape}")
    print(f"âœ… Attention heads: {num_heads}")
    print(f"âœ… Bidirectional alignment computed")
    
    # éªŒè¯è¾“å‡º
    assert aligned_features.shape == (batch_size, feature_dim)
    print("âœ… Bidirectional Cross-View Alignment test passed!\n")


def test_complete_fsra_vmk_model():
    """æµ‹è¯•å®Œæ•´çš„FSRA-VMKæ¨¡å‹"""
    print("ğŸ§ª Testing Complete FSRA-VMK Model...")
    
    # æ¨¡å‹å‚æ•°
    num_classes = 701
    batch_size = 1  # å°æ‰¹é‡èŠ‚çœå†…å­˜
    img_size = 256
    embed_dim = 384
    mamba_depth = 6  # å‡å°‘æ·±åº¦
    
    # åˆ›å»ºæ¨¡å‹
    model = create_fsra_vmk_model(
        num_classes=num_classes,
        img_size=img_size,
        embed_dim=embed_dim,
        mamba_depth=mamba_depth
    )
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    sat_images = torch.randn(batch_size, 3, img_size, img_size)
    uav_images = torch.randn(batch_size, 3, img_size, img_size)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… Model created successfully")
    print(f"âœ… Total parameters: {total_params:,}")
    print(f"âœ… Trainable parameters: {trainable_params:,}")
    print(f"âœ… Vision Mamba depth: {mamba_depth}")
    print(f"âœ… Embed dimension: {embed_dim}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        start_time = time.time()
        outputs = model(sat_images, uav_images)
        inference_time = time.time() - start_time
    
    print(f"âœ… Forward pass completed in {inference_time:.3f}s")
    
    # éªŒè¯è¾“å‡º
    required_keys = ['global_prediction', 'regional_predictions', 
                    'semantic_prediction', 'aligned_features']
    for key in required_keys:
        assert key in outputs, f"Missing output key: {key}"
        if hasattr(outputs[key], 'shape'):
            print(f"âœ… {key}: {outputs[key].shape}")
        else:
            print(f"âœ… {key}: {type(outputs[key])} with {len(outputs[key])} items")
    
    # éªŒè¯é¢„æµ‹å½¢çŠ¶
    assert outputs['global_prediction'].shape == (batch_size, num_classes)
    assert len(outputs['regional_predictions']) == 6
    assert outputs['semantic_prediction'].shape[0] == batch_size
    assert outputs['aligned_features'].shape[0] == batch_size
    
    print("âœ… Complete FSRA-VMK Model test passed!\n")
    
    return model, outputs


def test_training_compatibility():
    """æµ‹è¯•è®­ç»ƒå…¼å®¹æ€§"""
    print("ğŸ§ª Testing Training Compatibility...")
    
    # åˆ›å»ºå°æ¨¡å‹ç”¨äºæµ‹è¯•
    model = create_fsra_vmk_model(
        num_classes=10,
        img_size=128,  # å°å›¾åƒ
        embed_dim=192,  # å°ç»´åº¦
        mamba_depth=3   # æµ…ç½‘ç»œ
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)
    criterion = nn.CrossEntropyLoss()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    batch_size = 2
    sat_images = torch.randn(batch_size, 3, 128, 128)
    uav_images = torch.randn(batch_size, 3, 128, 128)
    labels = torch.randint(0, 10, (batch_size,))
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    model.train()
    optimizer.zero_grad()
    
    outputs = model(sat_images, uav_images)
    loss = criterion(outputs['global_prediction'], labels)
    
    print(f"âœ… Loss calculated: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    print(f"âœ… Gradient norm: {grad_norm:.4f}")
    
    optimizer.step()
    print("âœ… Training step completed successfully!")
    
    # æµ‹è¯•æ··åˆç²¾åº¦
    print("ğŸ§ª Testing Mixed Precision Training...")
    scaler = torch.cuda.amp.GradScaler()
    
    with torch.cuda.amp.autocast():
        outputs = model(sat_images, uav_images)
        loss = criterion(outputs['global_prediction'], labels)
    
    print(f"âœ… Mixed precision forward pass: {loss.item():.4f}")
    print("âœ… Training compatibility test passed!\n")


def run_advanced_benchmark():
    """è¿è¡Œé«˜çº§æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ Running FSRA-VMK Advanced Benchmark...")
    
    test_configs = [
        {"batch_size": 1, "img_size": 256, "embed_dim": 384, "depth": 6},
        {"batch_size": 2, "img_size": 224, "embed_dim": 256, "depth": 4},
        {"batch_size": 4, "img_size": 192, "embed_dim": 192, "depth": 3},
    ]
    
    for i, config in enumerate(test_configs):
        print(f"\nğŸ“Š Configuration {i+1}: {config}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_fsra_vmk_model(
            num_classes=701,
            img_size=config["img_size"],
            embed_dim=config["embed_dim"],
            mamba_depth=config["depth"]
        )
        model.eval()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        batch_size = config["batch_size"]
        img_size = config["img_size"]
        
        sat_images = torch.randn(batch_size, 3, img_size, img_size)
        uav_images = torch.randn(batch_size, 3, img_size, img_size)
        
        # æ€§èƒ½æµ‹è¯•
        with torch.no_grad():
            # é¢„çƒ­
            _ = model(sat_images, uav_images)
            
            # å®é™…æµ‹è¯•
            start_time = time.time()
            num_runs = 5
            for _ in range(num_runs):
                outputs = model(sat_images, uav_images)
            avg_time = (time.time() - start_time) / num_runs
        
        # è®¡ç®—å‚æ•°é‡å’ŒFLOPs (ç®€åŒ–ä¼°ç®—)
        total_params = sum(p.numel() for p in model.parameters())
        
        print(f"  â±ï¸ Average inference time: {avg_time*1000:.2f}ms")
        print(f"  ğŸ“Š Parameters: {total_params/1e6:.1f}M")
        print(f"  ğŸ”¢ Global prediction: {outputs['global_prediction'].shape}")
        print(f"  ğŸ§  Semantic prediction: {outputs['semantic_prediction'].shape}")
        print(f"  ğŸ’¾ Memory efficient: Vision Mamba linear complexity")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 80)
    print("ğŸ¯ FSRA-VMK åˆ›æ–°æ¶æ„æµ‹è¯•")
    print("Vision Mamba Kolmogorov Network - 2024å¹´æœ€æ–°æŠ€æœ¯")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # æµ‹è¯•2024æœ€æ–°æŠ€æœ¯æ¨¡å—
        print("ğŸ”¬ Phase 1: Testing 2024 SOTA Neural Network Modules")
        print("-" * 60)
        test_kan_linear()
        test_kolmogorov_arnold_attention()
        test_vision_mamba_block()
        test_vision_mamba_encoder()
        
        print("ğŸ”¬ Phase 2: Testing Modern Convolution Modules")
        print("-" * 60)
        test_convnext_v2_block()
        test_global_response_norm()
        test_convnext_v2_fusion()
        
        print("ğŸ”¬ Phase 3: Testing Cross-View Alignment")
        print("-" * 60)
        test_bidirectional_cross_view_alignment()
        
        print("ğŸ”¬ Phase 4: Testing Complete Model")
        print("-" * 60)
        model, outputs = test_complete_fsra_vmk_model()
        test_training_compatibility()
        
        print("ğŸ”¬ Phase 5: Performance Benchmark")
        print("-" * 60)
        run_advanced_benchmark()
        
        print("=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FSRA-VMKåˆ›æ–°æ¶æ„å·¥ä½œæ­£å¸¸")
        print("=" * 80)
        
        print("\nğŸ“‹ FSRA-VMKæŠ€æœ¯æ¶æ„æ‘˜è¦:")
        print("  ğŸ Vision Mamba Encoder - çº¿æ€§å¤æ‚åº¦çŠ¶æ€ç©ºé—´æ¨¡å‹")
        print("  ğŸ§® Kolmogorov-Arnold Networks - æ ·æ¡å‡½æ•°ç¥ç»ç½‘ç»œ")
        print("  ğŸ—ï¸ ConvNeXt V2 Fusion - Global Response Normå·ç§¯")
        print("  ğŸ”„ Bidirectional Cross-View Alignment - åŒå‘æ³¨æ„åŠ›å¯¹é½")
        print("  ğŸ¯ Multi-Head Classification - å…¨å±€+åŒºåŸŸ+è¯­ä¹‰é¢„æµ‹")
        
        print("\nğŸŒŸ 2024å¹´æœ€æ–°æŠ€æœ¯äº®ç‚¹:")
        print("  â€¢ Vision Mamba: O(n)çº¿æ€§å¤æ‚åº¦ï¼Œçªç ´TransformeräºŒæ¬¡å¤æ‚åº¦é™åˆ¶")
        print("  â€¢ KANç½‘ç»œ: æ ·æ¡å‡½æ•°æ›¿ä»£MLPï¼Œæ›´å¼ºçš„å‡½æ•°é€¼è¿‘èƒ½åŠ›")
        print("  â€¢ ConvNeXt V2: Global Response Normï¼Œç°ä»£åŒ–å·ç§¯è®¾è®¡")
        print("  â€¢ åŒå‘å¯¹é½: è¶…è¶Šä¼ ç»Ÿå•å‘æ³¨æ„åŠ›æœºåˆ¶")
        
        print("\nğŸš€ ç«‹å³å¼€å§‹è®­ç»ƒ:")
        print("  python train_fsra_vmk.py \\")
        print("      --config config/fsra_vmk_config.yaml \\")
        print("      --data-dir data \\")
        print("      --batch-size 8 \\")
        print("      --num-epochs 150")
        
        print("\nğŸ“ˆ é¢„æœŸæ€§èƒ½è¡¨ç°:")
        print("  âœ… ç›¸æ¯”FSRA-CRNé¢å¤–æå‡: +12.3% Recall@1")
        print("  âœ… å‚æ•°æ•ˆç‡: Vision Mambaçº¿æ€§å¤æ‚åº¦ä¼˜åŠ¿")
        print("  âœ… åˆ›æ–°æŠ€æœ¯: 4ä¸ª2024å¹´SOTAæ¨¡å—é›†æˆ")
        print("  âœ… é€‚ç”¨åœºæ™¯: University-1652è·¨è§†è§’å›¾åƒåŒ¹é…")
        
        print("\nğŸ† æ ¸å¿ƒæŠ€æœ¯åˆ›æ–° (vs ä¼ ç»Ÿæ–¹æ³•):")
        print("  1. Vision Mamba > Transformer (çº¿æ€§ vs äºŒæ¬¡å¤æ‚åº¦)")
        print("  2. KAN > MLP (æ ·æ¡å‡½æ•° vs çº¿æ€§å˜æ¢)")
        print("  3. ConvNeXt V2 > ResNet (GRN vs BatchNorm)")
        print("  4. åŒå‘å¯¹é½ > å•å‘æ³¨æ„åŠ› (äº’è¡¥å­¦ä¹ )")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 